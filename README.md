[GAN_blocks.md](https://github.com/user-attachments/files/24154385/GAN_blocks.md)–ó–∞–¥–∞–Ω–∏–µ 8: GAN –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
=
### –ó–∞–¥–∞—á–∞: —Å–æ–∑–¥–∞—Ç—å Generative Adversarial Network –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ü–∏—Ñ—Ä.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:

Generator: Dense —Å–ª–æ–∏ + Reshape + Conv2DTranspose

Discriminator: Conv2D —Å–ª–æ–∏ + Flatten + Dense

45

Minimax loss –¥–ª—è —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batch normalization

### –ß—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å:

1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Generator —Å Conv2DTranspose

 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Discriminator —Å Conv2D

 3. Batch normalization —Å–ª–æ–∏

 4. –§—É–Ω–∫—Ü–∏—é train_step —Å –æ–±—É—á–µ–Ω–∏–µ–º –æ–±–µ–∏—Ö —Å–µ—Ç–µ–π

 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

 6. –ì—Ä–∞—Ñ–∏–∫–∏ loss –∫—Ä–∏–≤—ã—Ö

–ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import json
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
```

–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø GAN
```python
class GAN:
    """Generative Adversarial Network –¥–ª—è MNIST"""
    
    def __init__(self, latent_dim=100):
        self.latent_dim = latent_dim
        self.generator = self._build_generator()
        self.discriminator = self._build_discriminator()
        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)
        
        self.g_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
        self.d_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
        
        self.d_losses = []
        self.g_losses = []
```

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ì–ï–ù–ï–†–ê–¢–û–†–ê
```python
def _build_generator(self):
    model = keras.Sequential([
        layers.Input(shape=(self.latent_dim,)),
        
        layers.Dense(256),
        layers.BatchNormalization(),
        layers.ReLU(),
        
        layers.Dense(512),
        layers.BatchNormalization(),
        layers.ReLU(),
        
        layers.Dense(1024),
        layers.BatchNormalization(),
        layers.ReLU(),
        
        layers.Dense(7*7*64),
        layers.BatchNormalization(),
        layers.Reshape((7, 7, 64)),
        
        layers.Conv2DTranspose(32, kernel_size=(4, 4), strides=(2, 2), padding='same'),
        layers.BatchNormalization(),
        layers.ReLU(),
        
        layers.Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same'),
        layers.Activation('tanh')
    ])
    return model
```

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –î–ò–°–ö–†–ò–ú–ò–ù–ê–¢–û–†–ê
```python
def _build_discriminator(self):
    model = keras.Sequential([
        layers.Input(shape=(28, 28, 1)),
        
        layers.Conv2D(32, (3, 3), padding='same'),
        layers.LeakyReLU(alpha=0.2),
        layers.MaxPooling2D((2, 2)),
        
        layers.Conv2D(64, (3, 3), padding='same'),
        layers.BatchNormalization(),
        layers.LeakyReLU(alpha=0.2),
        layers.MaxPooling2D((2, 2)),
        
        layers.Conv2D(128, (3, 3), padding='same'),
        layers.BatchNormalization(),
        layers.LeakyReLU(alpha=0.2),
        
        layers.Flatten(),
        layers.Dense(512),
        layers.LeakyReLU(alpha=0.2),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    return model
```

–ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–û–í
```python
def verify_shapes(self):
    print("\nüìè –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–û–í:")
    print("=" * 60)
    
    test_noise = tf.random.normal([1, self.latent_dim])
    gen_output = self.generator(test_noise, training=False)
    
    print(f"‚úì Generator input:  (1, {self.latent_dim})")
    print(f"‚úì Generator output: {gen_output.shape}")
    
    if gen_output.shape != (1, 28, 28, 1):
        print(f"‚ùå –û–®–ò–ë–ö–ê! Generator –¥–æ–ª–∂–µ–Ω –≤—ã–≤–æ–¥–∏—Ç—å (1, 28, 28, 1)")
        return False
    
    disc_output = self.discriminator(gen_output, training=False)
    
    print(f"‚úì Discriminator input:  {gen_output.shape}")
    print(f"‚úì Discriminator output: {disc_output.shape}")
    
    if disc_output.shape != (1, 1):
        print(f"‚ùå –û–®–ò–ë–ö–ê! Discriminator –¥–æ–ª–∂–µ–Ω –≤—ã–≤–æ–¥–∏—Ç—å (1, 1)")
        return False
    
    print("=" * 60)
    print("‚úÖ –í–°–ï –†–ê–ó–ú–ï–†–´ –ü–†–ê–í–ò–õ–¨–ù–´–ï!\n")
    return True
```

–û–î–ò–ù –®–ê–ì –û–ë–£–ß–ï–ù–ò–Ø
```python
@tf.function
def train_step(self, real_images):
    batch_size = tf.shape(real_images)[0]
    
    with tf.GradientTape() as tape:
        noise = tf.random.normal([batch_size, self.latent_dim])
        fake_images = self.generator(noise, training=True)
        
        real_predictions = self.discriminator(real_images, training=True)
        fake_predictions = self.discriminator(fake_images, training=True)
        
        real_loss = self.loss_fn(tf.ones_like(real_predictions), real_predictions)
        fake_loss = self.loss_fn(tf.zeros_like(fake_predictions), fake_predictions)
        d_loss = real_loss + fake_loss
    
    d_gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)
    self.d_optimizer.apply_gradients(
        zip(d_gradients, self.discriminator.trainable_weights)
    )
    
    with tf.GradientTape() as tape:
        noise = tf.random.normal([batch_size, self.latent_dim])
        fake_images = self.generator(noise, training=True)
        fake_predictions = self.discriminator(fake_images, training=True)
        g_loss = self.loss_fn(tf.ones_like(fake_predictions), fake_predictions)
    
    g_gradients = tape.gradient(g_loss, self.generator.trainable_weights)
    self.g_optimizer.apply_gradients(
        zip(g_gradients, self.generator.trainable_weights)
    )
    
    return d_loss, g_loss
```

–¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø
```python
def train(self, X_train, epochs=50, batch_size=128):
    train_dataset = tf.data.Dataset.from_tensor_slices(X_train)
    train_dataset = train_dataset.shuffle(buffer_size=10000)
    train_dataset = train_dataset.batch(batch_size)
    
    print("=" * 70)
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø GAN")
    print("=" * 70)
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   ‚Ä¢ –≠–ø–æ—Ö–∏: {epochs}")
    print(f"   ‚Ä¢ Batch size: {batch_size}")
    print(f"   ‚Ä¢ –î–∞—Ç–∞—Å–µ—Ç: {len(X_train)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print("=" * 70)
    
    for epoch in range(epochs):
        epoch_d_loss = []
        epoch_g_loss = []
        
        for real_images in train_dataset:
            d_loss, g_loss = self.train_step(real_images)
            epoch_d_loss.append(float(d_loss))
            epoch_g_loss.append(float(g_loss))
        
        avg_d_loss = np.mean(epoch_d_loss)
        avg_g_loss = np.mean(epoch_g_loss)
        
        self.d_losses.append(avg_d_loss)
        self.g_losses.append(avg_g_loss)
        
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}/{epochs} - D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}")
    
    print("=" * 70)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)
```

–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ò–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
```python
def generate_images(self, num_images=10):
    noise = tf.random.normal([num_images, self.latent_dim])
    return self.generator(noise, training=False)
```

–≠–ö–°–ü–û–†–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
```python
def export_results(self, filename='gan_results.json'):
    results = {
        'epochs': len(self.d_losses),
        'd_losses': [float(x) for x in self.d_losses],
        'g_losses': [float(x) for x in self.g_losses],
        'learning_rate': 0.0002,
        'batch_size': 128,
        'dataset_size': 10000,
        'timestamp': datetime.now().isoformat()
    }
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
```

–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
```python
def save_models(self):
    self.generator.save('generator.h5')
    self.discriminator.save('discriminator.h5')
    print("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: generator.h5, discriminator.h5")
```

–ì–†–ê–§–ò–ö –ü–û–¢–ï–†–¨
```python
def plot_losses(d_losses, g_losses):
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    fig.suptitle('GAN Training Analysis', fontsize=16, fontweight='bold')
    
    ax = axes[0, 0]
    ax.plot(d_losses, label='Discriminator Loss', color='#FF6B6B', linewidth=2)
    ax.set_xlabel('Epoch', fontsize=11)
    ax.set_ylabel('Loss', fontsize=11)
    ax.set_title('Discriminator Loss (Raw)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.legend(fontsize=10)
    
    ax = axes[0, 1]
    ax.plot(g_losses, label='Generator Loss', color='#4ECDC4', linewidth=2)
    ax.set_xlabel('Epoch', fontsize=11)
    ax.set_ylabel('Loss', fontsize=11)
    ax.set_title('Generator Loss (Raw)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.legend(fontsize=10)
    
    ax = axes[1, 0]
    window = max(3, len(d_losses) // 10)
    d_smooth = pd.Series(d_losses).rolling(window=window, center=True).mean()
    g_smooth = pd.Series(g_losses).rolling(window=window, center=True).mean()
    
    ax.plot(d_smooth, label='D Loss (Smoothed)', color='#FF6B6B', linewidth=2.5)
    ax.plot(g_smooth, label='G Loss (Smoothed)', color='#4ECDC4', linewidth=2.5)
    ax.set_xlabel('Epoch', fontsize=11)
    ax.set_ylabel('Loss', fontsize=11)
    ax.set_title('Both Losses (Smoothed)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.legend(fontsize=10)
    
    ax = axes[1, 1]
    ax.axis('off')
    
    d_improvement = ((d_losses[-1] - d_losses[0]) / d_losses[0] * 100)
    g_improvement = ((g_losses[-1] - g_losses[0]) / g_losses[0] * 100)
    
    stats_text = f"""
üìä TRAINING STATISTICS

Discriminator Loss:
  ‚Ä¢ Initial: {d_losses[0]:.4f}
  ‚Ä¢ Final:   {d_losses[-1]:.4f}
  ‚Ä¢ Average: {sum(d_losses)/len(d_losses):.4f}

Generator Loss:
  ‚Ä¢ Initial: {g_losses[0]:.4f}
  ‚Ä¢ Final:   {g_losses[-1]:.4f}
  ‚Ä¢ Improvement: {g_improvement:+.1f}%"""
    
    ax.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',
            verticalalignment='center', bbox=dict(boxstyle='round', 
            facecolor='wheat', alpha=0.3))
    
    plt.tight_layout()
    plt.savefig('gan_training_loss.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Loss graph saved to gan_training_loss.png")
    plt.show()
```

–ì–†–ê–§–ò–ö –û–ë–†–ê–ó–¶–û–í
```python
def plot_generated_samples(gan, num_samples=16):
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    fig.suptitle('Generated MNIST Digits', fontsize=14, fontweight='bold')
    
    generated = gan.generate_images(num_samples)
    generated = (generated.numpy() + 1) / 2
    
    for i, ax in enumerate(axes.flat):
        ax.imshow(generated[i].reshape(28, 28), cmap='gray')
        ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('generated_samples.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Generated samples saved to generated_samples.png")
    plt.show()
```

–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
```python
def load_and_preprocess_mnist():
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ MNIST –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    (X_train, _), _ = keras.datasets.mnist.load_data()
    X_train = X_train.astype(np.float32) / 127.5 - 1.0
    X_train = np.expand_dims(X_train, axis=-1)
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {X_train.shape}\n")
    return X_train
```

–ì–õ–ê–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê
```python
if __name__ == "__main__":
    
    print("\n")
    print("‚ïî" + "=" * 68 + "‚ïó")
    print("‚ïë" + " " * 15 + "ü§ñ GAN –¥–ª—è MNIST —Å –ì–†–ê–§–ò–ö–ê–ú–ò! ü§ñ" + " " * 21 + "‚ïë")
    print("‚ïö" + "=" * 68 + "‚ïù")
    print()
    
    X_train = load_and_preprocess_mnist()
    
    print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ GAN...")
    gan = GAN(latent_dim=100)
    
    if not gan.verify_shapes():
        print("‚ùå –û–®–ò–ë–ö–ê –í –ê–†–•–ò–¢–ï–ö–¢–£–†–ï!")
        exit(1)
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n")
    gan.train(X_train[:10000], epochs=50, batch_size=128)
    
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    gan.export_results('gan_results.json')
    gan.save_models()
    
    print("\n" + "=" * 70)
    print("üìà –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ò–ö–û–í")
    print("=" * 70)
    
    print("\n1Ô∏è‚É£ –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è...")
    plot_losses(gan.d_losses, gan.g_losses)
    
    print("\n2Ô∏è‚É£ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏—Ñ—Ä—ã...")
    plot_generated_samples(gan, num_samples=16)
    
    print("\n" + "=" * 70)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–´!")
    print("=" * 70)
    print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("  ‚úì gan_results.json - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è")
    print("  ‚úì gan_training_loss.png - –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ—Ç–µ—Ä—å üìä")
    print("  ‚úì generated_samples.png - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏—Ñ—Ä—ã üé®")
    print("  ‚úì generator.h5 - –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞")
    print("  ‚úì discriminator.h5 - –º–æ–¥–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞")
    print("\n‚ú® –ó–∞–≥—Ä—É–∑–∏—Ç–µ gan_results.json –≤ GAN_Browser_App.html –¥–ª—è –≤–µ–±-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏!")
    print("=" * 70)
```
 
# –û—Ç–≤–µ—Ç –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –Ω–æ–º–µ—Ä 8

### –û–ø–∏—à–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º Graham Scan –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—ã–ø—É–∫–ª–æ–π –æ–±–æ–ª–æ—á–∫–∏. –ö–∞–∫–æ–≤–∞ –µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å?

### Graham Scan 
‚Äî –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ–π –æ–±–æ–ª–æ—á–∫–∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫ –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏. –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–¥–µ–µ "–æ–±—Ö–æ–¥–∞" —Ç–æ—á–µ–∫ –≤ –ø–æ—Ä—è–¥–∫–µ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—è –ø–æ–ª—è—Ä–Ω–æ–≥–æ —É–≥–ª–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫—Ä–∞–π–Ω–µ–π —Ç–æ—á–∫–∏.

–®–∞–≥–∏:

–ù–∞–π—Ç–∏ —Ç–æ—á–∫—É P0 —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π y (–ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö y ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é x)

–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–æ–ª—è—Ä–Ω–æ–º—É —É–≥–ª—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ P0

–ü–æ–º–µ—Å—Ç–∏—Ç—å P0 –∏ –ø–µ—Ä–≤—ã–µ 2 —Ç–æ—á–∫–∏ –≤ —Å—Ç–µ–∫

–î–ª—è –∫–∞–∂–¥–æ–π —Å–ª–µ–¥—É—é—â–µ–π —Ç–æ—á–∫–∏:

–ü–æ–∫–∞ 3 –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–æ—á–∫–∏ —Å—Ç–µ–∫–∞ –æ–±—Ä–∞–∑—É—é—Ç –Ω–µ –ª–µ–≤—ã–π –ø–æ–≤–æ—Ä–æ—Ç (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ ‚â§ 0)

–£–¥–∞–ª–∏—Ç—å –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É

–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—É—â—É—é —Ç–æ—á–∫—É

–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(n log n) 

